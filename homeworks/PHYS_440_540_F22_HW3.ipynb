{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Problems\n",
    "## Due Friday 14 October, by 5pm\n",
    "\n",
    "### PHYS 440/540, Fall 2022\n",
    "https://github.com/gtrichards/PHYS_440_540/\n",
    "\n",
    "\n",
    "## Problem 1\n",
    "On Data Camp:\n",
    "\n",
    "Statistical Thinking in Python (Part 2): Bootstrap confidence intervals \n",
    "\n",
    "This is Chapter 2.  You shouldn't need Chapter 1, but you are welcome to do it.  If this chapter interests you, then you might get something out of completing Chapters 3 and 4 as well.\n",
    "\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "We're going to do some polynomial fits to data just like in the lecture. However, in all cases you should keep the y-intercept fixed at -0.23.\n",
    "\n",
    "1) Read in data/hw3data.npy.  Set x, y, sigma_y = data.\n",
    "\n",
    "2) Use the following code to compute the un-normalized posterior pdf (i.e. just the likelihood x prior) on a grid of the linear coefficient (i.e. the slope) of a linear model, with a uniform prior between 0.5 and 1.5. Plot this posterior pdf. Remember this is just a one-dimensional model because the intercept is fixed. Use a grid size of 100.\n",
    "\n",
    "3) Now compute the joint 2D posterior pdf (again just the likelihood x prior) of linear and quadratic coefficients of a quadratic model. Give the linear coefficient a uniform prior between 0.5 and 1.5. Give the quadratic coefficient a uniform prior between -1 and 0.25. Plot this two-dimensional posterior. Remember this is a two-dimensional model because the intercept is fixed. Use a grid size of 100 in each model dimension.\n",
    "\n",
    "4) Integrate the posterior pdfs and take the ratio to calculate the Bayes factor for a linear versus quadratic model. How does this compare/contrast with the BIC model comparison in the lecture?\n",
    "\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "Produce the \"trace\" plot for the clear/cloudy day problem from Inference2.ipynb.  Then plot the histogram of that trace, throwing away steps in the trace that could be considered part of the burn-in.  Your plots should look like the ones from class (or nicer!).  \n",
    "\n",
    "* Do you get (essentially) the same answer for day $N$ as day $N+1$?\n",
    "* Does it matter whether is was clear or cloudy on the day that you started? \n",
    "* What length \"burn in\" would you recommend for this process?\n",
    "\n",
    "See starter code below.  You don't **have** to use it (e.g., if you have a different way to approach/code the problem), but it should help you get started.\n",
    "\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "Copy over all the code cells from our interactive MCMC example in Inference2 (7 cells in all from the cell that imports numpy to the histogram of the chain).  \n",
    "\n",
    "Change the code so that the likelihood plot is drawn in purple instead of green or red when the odds ratio for the next model is not favored, but is still larger than the random value used for the \"accept\" threshold.  So, now you will have green plots for steps that are both better and accepted, purple for steps that aren't better but are still accepted, and red for steps that are not accepted.  Also change the code so that the step size is small enough that you are likely to take a long time to reach the most likely value.\n",
    "\n",
    "Once you have done both of those things, then:\n",
    "\n",
    "* Run enough steps to show 2 steps that were accepted, even though they are worse.\n",
    "* Plot the chain showing it taking some time to get from the initial value to oscillating around the most likely value.\n",
    "* Plot a histogram of the chain both using the full chain and after throwing away the \"burn\" period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 Starter Code\n",
    "\n",
    "I haven't written the solution yet, but will post some starter code once I have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.load(\"../data/hw3data.npy\")\n",
    "____, ____, ____ = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to do a polynomial fit, and compute the likelihood\n",
    "def polynomial_fit(theta, x):\n",
    "    \"\"\"Polynomial model of degree (len(theta) - 1)\"\"\"\n",
    "    # For a polynomial with order 1, this gives theta_0 + theta_1*x\n",
    "    # For a polynomial with order 2, this gives theta_0 + theta_1*x + theta_2*x^2, etc.\n",
    "    return sum(t * x ** n for (n, t) in enumerate(theta))\n",
    "\n",
    "# compute the data log-likelihood given a model\n",
    "def logL(theta, data, model=polynomial_fit):\n",
    "    \"\"\"Gaussian log-likelihood of the model at theta\"\"\"\n",
    "    x, y, sigma_y = data\n",
    "    y_fit = model(theta, x)\n",
    "    return sum(scipy.stats.norm.logpdf(*args) \n",
    "               for args in zip(y, y_fit, sigma_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = ____/(____-____) #Uniform prior between 0.5 and 1.5\n",
    "\n",
    "#Grid for slopes of linear fit\n",
    "xgrid = np.linspace(____, ____, ____)\n",
    "\n",
    "#Use the midpoint of the bins instead of the bin edges\n",
    "xgrid_mid = 0.5*(xgrid[1:] + xgrid[:-1])\n",
    "\n",
    "#y-intecept is fixed\n",
    "theta_0 = [____]\n",
    "\n",
    "#Compute the log likelihood for each value of the slope and (fixed) intercept\n",
    "logLarray = np.array([logL(np.append(____,____), data, model=polynomial_fit) for theta_1 in xgrid_mid])\n",
    "\n",
    "#We were asked about the posterior (remember log rules for multiplication)\n",
    "postarray = np.log10(____) + ____\n",
    "\n",
    "#Plot the posterior (unlog to see actual values)\n",
    "plt.plot(____, np.exp(____))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior1 = ____/____ #Uniform prior between 0.5 and 1.5\n",
    "prior2 = ____/____ #Uniform prior between -1 and 0.25\n",
    "\n",
    "#Grid for coefficient of linear term\n",
    "xgrid1 = ____\n",
    "xgrid1_mid = ____\n",
    "\n",
    "#Grid for coefficient of quadratic term\n",
    "xgrid2 = ____\n",
    "xgrid2_mid = ____\n",
    "\n",
    "postarray2D = np.zeros((xgrid1_mid.shape[0],xgrid2_mid.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,theta_1 in enumerate(xgrid1_mid):\n",
    "    for j,theta_2 in enumerate(xgrid2_mid):\n",
    "        theta = [____, ____, ____]\n",
    "        \n",
    "        logLarray= logL(np.array(____), data, model=polynomial_fit)\n",
    "        \n",
    "        postarray2D[____,____] = np.log10(____) + np.log10(____) + _____\n",
    "                             \n",
    "postarray2D = np.exp(postarray2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(____, extent =[xgrid2_mid.min(), xgrid2_mid.max(),\n",
    "                               xgrid1_mid.min(), xgrid1_mid.max()]);\n",
    "plt.xlabel('quadratic term')\n",
    "plt.ylabel('linear term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.sum(np.exp(____))*np.diff(xgrid)[0]\n",
    "z2 = np.sum(____)*np.diff(____)[0]*np.diff(____)[0]\n",
    "\n",
    "#Odds ratio is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N=____ # Number of days to run the simulation\n",
    "today = ____ # Set weather on the first day: clear=1 cloudy=0\n",
    "\n",
    "tomorrow = ____ # Initialize tomorrow to some null value\n",
    "cleardays = 0 # Keep track of the total number of clear days\n",
    "\n",
    "chain = []\n",
    "chain = np.array([])\n",
    "pclearall = np.array([])\n",
    "t = np.arange(____)\n",
    "for j in np.arange(____):\n",
    "\n",
    "    if (today):\n",
    "        ____ #Increament the number of clear days if today was claer\n",
    "    \n",
    "    # Add value for today to our Markov Chain\n",
    "    chain = np.append(____,____)\n",
    "    \n",
    "    # Fraction of days that have been clear so far\n",
    "    pclearall = np.append(pclearall,1.0*cleardays/(j+1))\n",
    "    \n",
    "    #Random number between 0 and 1 for tomorrow\n",
    "    p = np.____.____(____)\n",
    "\n",
    "    #clear today\n",
    "    if (_____):\n",
    "        if (p<=0.9):\n",
    "            ____ = ____\n",
    "        else:\n",
    "            ____ = ____ \n",
    "            \n",
    "    #cloudy today\n",
    "    else:\n",
    "        if (p<=0.5):\n",
    "            _____ = _____\n",
    "        else:\n",
    "            ____ = ____\n",
    "\n",
    "    #Tomorrow will become today\n",
    "    today=tomorrow\n",
    "        \n",
    "\n",
    "#Final value for fraction of clear days        \n",
    "pclearfinal = 1.0*cleardays/N\n",
    "print(pclearfinal)\n",
    "print(chain)\n",
    "        \n",
    "plt.plot(t, pclearall)\n",
    "plt.ylim(____,____)\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('P(clear)')\n",
    "plt.savefig('Chain.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a histogram of `pclearall` with and without the \"burn\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 Starter Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy over the code cells for the MCMC example in Inference2, starting with this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so on for the rest of the cells in this example.  Change the cells as needed based on the questions posed."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
