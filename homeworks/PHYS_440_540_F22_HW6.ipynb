{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6: Problems\n",
    "## Due Wednesday 2 November, by 11:59pm\n",
    "\n",
    "### PHYS 440/540, Fall 2022\n",
    "https://github.com/gtrichards/PHYS_440_540/\n",
    "\n",
    "\n",
    "## Problems 1\n",
    "\n",
    "Complete Chapter 4 of the *dimensionality reduction* course in Data Camp.\n",
    "\n",
    "## Problem 2&3\n",
    "\n",
    "Do 2 more Data Camp chapters of your choice from within the following:\n",
    "* Interactive Data Visualization with Bokeh\n",
    "* Dimensionality reduction\n",
    "* Introduction to Data Visualization in Python\n",
    "* Introduction to Data Visualization with Matplotlib\n",
    "* Introduction to Data Visualization with Seaborn\n",
    "* Intermediate Data Visualization with Seaborn\n",
    "* Data Visualization for Everyone\n",
    "\n",
    "I'm not going to assign all of them as that will just cause confusion (in that you might think that I'm assigning 20 hours of work!).  So, you are just going to tell me which 2 chapters you completed from among all the choices in this list of courses.  Do this by filling in the Course/Chapter title in the cell below.  Scroll through the course outlines first to see which ones look the most interesting/useful to you.\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "1. Read in a sample of SDSS imaging data.  Print out the feature names, and create a data matrix out of a subset of features (at least 10, but otherwise your choice [but don't choose `type` or `run`]).\n",
    "\n",
    "2. Use PCA (with randomized svd_solver for speed) to reduce the data matrix down to $2$ features. What is the explained variance of the data encapsulated in these eigen-features?\n",
    "\n",
    "3. Access the `type` key of the `data` structure and make an array of labels out of these. \n",
    "\n",
    "4. Make a scatter plot of the PCA-reduced data for these $5000$ random samples, colored by their corresponding `type`. *(You may want to set the transparency to be lower than 1 to see the mixing of samples.)*\n",
    "\n",
    "5. Choose $5000$ random integers between 0 and the number of samples in the data matrix. Record these integers because you'll use them later. \n",
    "\n",
    "6. Now try some non-linear dimensional reduction. These algorithms are slower than PCA, so you will operate only on the $5000$ random samples identified in the previous part. \n",
    "   - Try `LocallyLinearEmbedding`, `Isomap`, and `TSNE` algorithms, setting the number of components to be $2$ in all cases. \n",
    "\n",
    "7. As in the PCA case, make scatter plots of the dimensionally-reduced data, color coded by their `type`. For LLE and Isomap, experiment with the number of nearest neighbors between $5$ and $100$ to see what visually gives the best separation in `type` populations. For TSNE, do the same for the perplexity attribute. \n",
    "\n",
    "8. Which algorithm gives the cleanest way to visually see the two populations of sources? *(This will be subjective according to the samples you trained on, and even the randomness of the algorithms.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapters completed for Problems 2 & 3:\n",
    "* 2: \n",
    "* 3: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 4 Starter Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read in a sample of SDSS imaging data.  Print out the feature names, and create a data matrix out of a subset of the features (at least 10, but otherwise your choice [but don't choose `type` or `run`])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from astroML.datasets import fetch_imaging_sample\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ____ \n",
    "data.shape  # number of objects in dataset\n",
    "\n",
    "print(data.dtype.names)\n",
    "\n",
    "keylist = [____]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack([data[key] for ____ in ____]) \n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use PCA (with randomized svd_solver for speed) to reduce the data matrix down to $2$ features. What is the explained variance of the data encapsulated in these eigen-features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Randomized PCA for speed\n",
    "pca = PCA(____, svd_solver='____')\n",
    "X_reduced = pca.____(____)\n",
    "evals = pca.____\n",
    "print('Expained fractional variance of data encapsulated in the eigenvalues: ' + ____)\n",
    "print('Total explained variance: ' + ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Access the `type` key of the `data` structure and make an array of labels out of these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(data[____]))\n",
    "#Note one of these types refers to stars, the other to galaxies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Make a scatter plot of the PCA-reduced data for these $5000$ random samples, colored by their corresponding `type`. *(You may want to set the transparency to be lower than 1 to see the mixing of samples.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(____, ____, c=data[____], alpha=____)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Choose $5000$ random integers between 0 and the number of samples in the data matrix. Record these integers because you'll use them later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.random.____(low=____,high=____,size=____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now try some non-linear dimensional reduction. These algorithms are slower than PCA, so you will operate only on the $5000$ random samples identified in the previous part. \n",
    "   - Try `LocallyLinearEmbedding`, `Isomap`, and `TSNE` algorithms, setting the number of components to be $2$ in all cases. \n",
    "   \n",
    "7. As in the PCA case, make scatter plots of the dimensionally-reduced data, color coded by their `type`. For LLE and Isomap, experiment with the number of nearest neighbors between $5$ and $100$ to see what visually gives the best separation in `type` populations. For TSNE, do the same for the perplexity attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import ____\n",
    "k = ____ # Number of neighbors to use in fit\n",
    "n = ____ # Number of dimensions to fit\n",
    "\n",
    "lle = ____(____,____)\n",
    "\n",
    "X_reduced = ____.____(X[ind,:])\n",
    "\n",
    "plt.scatter(____,____,____,____)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import ____\n",
    "k = ____\n",
    "n = ____\n",
    "\n",
    "iso = ____(____,____)\n",
    "\n",
    "%etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import ____\n",
    "n = ____\n",
    "\n",
    "tsne = ____(____,perplexity=____)\n",
    "\n",
    "%etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "8. Which algorithm gives the cleanest way to visually see the two populations of sources? *(This will be subjective according to the samples you trained on, and even the randomness of the algorithms.)*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
